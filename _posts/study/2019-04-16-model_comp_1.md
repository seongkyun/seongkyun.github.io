---
layout: post
title: Model compression(Pruning/Distillation) ì •ë¦¬
category: study
tags: [Model compression, Pruning, Distillation]
comments: true
---

# Model compression(Pruning/Distillation) ì •ë¦¬

## Model Compression
- __Model compression?__ í° ëª¨ë¸ì„ ì‘ê²Œ ë§Œë“œëŠ” ë°©ë²•
  - ëª¨ë¸ì˜ íš¨ìœ¨ì„ ì¢‹ê²Œ í•˜ë„ë¡ redundancyë¥¼ ì¤„ì´ê±°ë‚˜ efficiencyë¥¼ ë†’ì´ëŠ” ë°©ì‹
- Pruning / Distillation
  - Pruning: ëª¨ë¸ì—ì„œ ì¤‘ìš”ë„ê°€ ë‚®ì€ ë‰´ëŸ°ì„ ì œê±°í•˜ì—¬ ì§ì ‘ì ì¸ ëª¨ë¸ í¬ê¸°ì˜ ê°ì†Œ
    - ëª¨ë¸ì˜ redundancyë¥¼ ì¤„ì´ëŠ” ë°©ë²•
    - ì¥ì : ëª¨ë¸ ìµœì í™”ì— ë”°ë¥¸ ì—°ì‚°ëŸ‰ ê°ì†Œ ë° ëª¨ë¸ì‚¬ì´ì¦ˆ ì¶•ì†Œìœ¨ì´ ìƒë‹¹í•¨
    - ë‹¨ì : ëª¨ë¸ í•™ìŠµ ë° pruning ê³¼ì •ì´ ë³µì¡í•˜ê³  ë§¤ìš° ì˜¤ë˜ ê±¸ë¦¬ë©°, taskë³„ë¡œ ë‹¤ë¥¸ ì ìš©ë°©ë²•ì„ í•„ìš”ë¡œí•¨

  - Distillation: ì‘ì€ ëª¨ë¸ì— í° ëª¨ë¸ì˜ ì •ë³´(knowledge)ë¥¼ ì „ë‹¬í•˜ì—¬ ì‘ì€ ëª¨ë¸ì˜ ì •í™•ë„ë¥¼ í–¥ìƒì‹œí‚´
    - ëª¨ë¸ì˜ efficiencyë¥¼ ë†’ì´ëŠ” ë°©ì‹
    - ì¥ì : Pre-trained modelì„ ì´ìš©í•´ ë¹ ë¥´ê²Œ í•™ìŠµì‹œí‚¬ ìˆ˜ ìˆìœ¼ë©° ë²”ìš©ì ìœ¼ë¡œ ì ìš© ê°€ëŠ¥
    - ë‹¨ì : ëª¨ë¸ ìµœì í™” ê°œë…ë³´ë‹¤ëŠ” ì¡´ì¬í•˜ëŠ” ëª¨ë¸ì˜ í‘œí˜„ë ¥(capacity)ì„ ìµœëŒ€í•œ ì‚¬ìš©í•˜ê²Œ ë˜ì–´ ì •í™•ë„ê°€ í–¥ìƒë˜ì–´ í° ëª¨ë¸ì„ ëŒ€ì²´í•  ìˆ˜ ìˆê²Œ ë˜ëŠ” ê°„ì ‘ì ì¸ ëª¨ë¸ ì‚¬ì´ì¦ˆ ì¶•ì†Œë°©ë²•

## Network Pruning
- ë„¤íŠ¸ì›Œí¬ì˜ ìˆ˜ë§ì€ íŒŒë¼ë¯¸í„°ì¤‘ ì¤‘ìš”í•˜ì§€ ì•Šì€ íŒŒë¼ë¯¸í„°ë“¤ì„ ì œê±°í•˜ëŠ” ë°©ë²•
  - ì–´ë– í•œ ê¸°ì¤€ì„ ì •í•˜ì—¬ rankë¥¼ ë§Œë“¤ë„ë¡ ë‰´ëŸ°ì„ ì •ë ¬(ranking)
  - Rankingì— ë”°ë¼ ì¼ì • thresholdë¥¼ ë„˜ì§€ ëª»í•˜ëŠ” ë‰´ëŸ°ì„ ì œê±°
    - ì´ ê³¼ì •ì—ì„œ ë’¤ì— ì—°ê²°ëœ ë‰´ëŸ°ì—ê¹Œì§€ ëª¨ë‘ ì˜í–¥ì„ ë¯¸ì¹˜ë¯€ë¡œ íŒŒë¼ë¯¸í„°ê°€ ë§ì´ ì œê±°ë¨(pruning)
    - Pruned ë„¤íŠ¸ì›Œí¬ì˜ ì •í™•ë„ëŠ” ì „ì—ë¹„í•´ ì¡°ê¸ˆ ê°ì†Œë¨
    - ë„ˆë¬´ ë§ì´ pruningì´ ë˜ë©´ ë„¤íŠ¸ì›Œí¬ê°€ ì†ìƒë˜ì–´ ì •í™•ë„ íšŒë³µì´ ë¶ˆê°€ëŠ¥
  - Pruning í›„ ë„¤íŠ¸ì›Œí¬ë¥¼ ì¬í•™ìŠµì‹œì¼œ ì •í™•ë„ë¥¼ íšŒë³µ
    - Interactive Pruning

<center>
<figure>
<img src="/assets/post_img/study/2019-04-13-pruning/fig1.png" alt="views">
<figcaption>Interactive pruning</figcaption>
</figure>
</center>

- Pruning Convolutional Neural Networks for Resource Efficient Inference
  - ICLR 2017 ë…¼ë¬¸(Nvidia)
  - Pre-trained VGG16ì˜ filterë¥¼ randomí•˜ê²Œ ê³¨ë¼ pruningí•˜ê³  validation setì—ì„œ cost functionì˜ ë³€í™”ë¥¼ ì œì¼ ì ê²Œ í•˜ëŠ” ë‰´ëŸ°ì˜ rankê°€ ë†’ë„ë¡ í•˜ì—¬ ë‚˜ì—´
  - ì‹¤í—˜ê²°ê³¼ VGG16 ë„¤íŠ¸ì›Œí¬ ëª¨ë¸ì‚¬ì´ì¦ˆ 10ë°°ì •ë„ ì¤„ì¼ ìˆ˜ ìˆì—ˆìŒ
    - Caltech Birds-200 dataset ì´ìš©

## Network Pruning on SSD
### Multi-layer Pruning Framework for Compressing Single Shot MultiBox Detector
- Singh, Pravendra, et al. 2019 IEEE Winter Conference on Applications of Computer Vision (WACV). IEEE, 2019.
- Single Shot Multi-Box Detector(SSD)ë¥¼ pruning
  - Object detection: PASCAL VOC datasetì— ëŒ€í•´ ì •í™•ë„ëŠ” ìœ ì§€í•˜ë©° 6.7~4.9ë°°ì˜ ëª¨ë¸ ì••ì¶•(VGG16 based)
  - Classification: CIFAR datasetì— ëŒ€í•´ ì •í™•ë„ëŠ” ìœ ì§€í•˜ë©° 125ë°°ì˜ ëª¨ë¸ ì••ì¶•(VGG16)
- 1. Sparsity induction: Pre-trained SSD ë¥¼ L1-normì´ ì ìš©ëœ loss functionìœ¼ë¡œ threshold ì´í•˜ì˜ weightë¥¼ 0ìœ¼ë¡œ ë§Œë“¤ì–´ pruning ë  layer ì§‘í•© ğ¿ì„ ë§Œë“¦
  - L1-normì—ëŠ” test setê³¼ val setì„ ì´ìš©í•¨
  - ê¸°ì¤€ì´ ë  thresholdëŠ” validation setì„ ì´ìš©í•˜ì—¬ í•´ë‹¹ ë ˆì´ì–´ì˜ ê°’ í‰ê· ì„ ê¸°ì¤€ìœ¼ë¡œ ì •í•¨
- 2. Filter selection: ì´ë ‡ê²Œ ì„ íƒëœ layer ì§‘í•© ğ¿ì˜ ë ˆì´ì–´ ğ‘™ì˜ ì¤‘ìš”ë„ë¥¼ í‰ê°€
  - ë ˆì´ì–´ ğ‘™ê³¼ ë‹¤ìŒ ë ˆì´ì–´ ğ‘™+1ê°„ì˜ filter sparsity staticsë¥¼ ì´ìš©í•˜ì—¬ ì¤‘ìš”ë„ í‰ê°€
  - ì´ ê³¼ì •ì—ì„œ ë ˆì´ì–´ lì—ì„œ ì¤‘ìš”í•˜ì§€ ì•Šë‹¤ê³  íŒë‹¨ë˜ëŠ” filterì˜ listë¥¼ ì–»ìŒ
  - Layer ì§‘í•© ğ¿ì— ëŒ€í•´ ëª¨ë‘ ë°˜ë³µìˆ˜í–‰
- 3. Pruning: ì•ì—ì„œ ì¤‘ìš”í•˜ì§€ ì•Šë‹¤ íŒë‹¨ëœ ë ˆì´ì–´ ğ‘™ê³¼ í•´ë‹¹ outputê³¼ ì—°ê²°ëœ layer ğ‘™+1ì„ ì œê±°
  - Layer ì§‘í•© ğ¿ì— ëŒ€í•´ ëª¨ë‘ ë°˜ë³µìˆ˜í–‰
- 4. Retraining: ì•ì—ì„œ pruningëœ ë„¤íŠ¸ì›Œí¬ë¥¼ original loss(SSD)ë¥¼ ì´ìš©í•´ ë–¨ì–´ì§„ ì •í™•ë„ë¥¼ ë³µì›(ì¬í•™ìŠµ)

### ì‹¤í—˜ ê²°ê³¼

<center>
<figure>
<img src="/assets/post_img/papers/2019-04-15-pruning_ssd/table1.jpg" alt="views">
</figure>
</center>

- ì‹¤í—˜ ê²°ê³¼ ì •í™•ë„ëŠ” ë¹„êµì  ë³´ì¡´í•˜ë©´ì„œ ë„¤íŠ¸ì›Œí¬ì˜ í¬ê¸°ëŠ” ë§¤ìš° í¬ê²Œ ì¤„ì–´ë“¤ê²Œ ë¨

<center>
<figure>
<img src="/assets/post_img/papers/2019-04-15-pruning_ssd/table3.jpg" alt="views">
<img src="/assets/post_img/papers/2019-04-15-pruning_ssd/table5.jpg" alt="views">
<figcaption></figcaption>
</figure>
</center>

- ë˜í•œ ì—¬íƒ€ ëª¨ë¸ì— ë¹„í•´ í›¨ì”¬ ë†’ì€ mAPë¡œ ì¶”ë¡ í•˜ë©´ì„œë„ í›¨ì”¬ ì‘ì€ íŒŒë¼ë¯¸í„°ìˆ˜ë¥¼ ê°€ì§
- SSD300ì˜ ê²½ìš° detection layerë¥¼ ë’·ë‹¨ì—ì„œ pruning í•˜ëŠ”ê²ƒì´ ì•ë‹¨ì—ì„œ í•˜ëŠ”ê²ƒë³´ë‹¤ ë” ì •í™•í•œ ì‹¤í—˜ ê²°ê³¼ë¥¼ ë³´ì´ëŠ” ê²ƒì„ í™•ì¸ í•  ìˆ˜ ìˆì—ˆëŠ”ë°, ì´ëŠ” êµ¬ì¡°ìƒ 4, 5, 6ë²ˆì§¸ detection layerì˜ parameter ìˆ˜ê°€ ë§¤ìš° ì ì§€ë§Œ í° ê°ì²´ë¥¼ ì°¾ëŠ”ë° ì£¼ìš”í•˜ê²Œ ì‘ìš©í•˜ê¸° ë•Œë¬¸ì„

## Network Distillation
- ê°œë°œì— ì‚¬ìš©ë˜ëŠ” ë”¥ëŸ¬ë‹ ë„¤íŠ¸ì›Œí¬ì™€ ì‹¤ì œ ì ìš©ë˜ëŠ” ëª¨ë¸ì€ ë‹¤ë¦„
  - ê°œë°œì—ëŠ” ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ì´ ììœ ë¡­ì§€ë§Œ, applicationë‹¨ì—ì„œëŠ” ê·¸ë ‡ì§€ ì•ŠìŒ
  - ì´ë¥¼ ìœ„í•´ í¬ê³  ë¬´ê±°ìš´ ëª¨ë¸ì˜ ì •ë³´(knowledge)ë¥¼ ì‘ê³  ê°€ë²¼ìš´ ëª¨ë¸ë¡œ ì „ë‹¬í•˜ì—¬ ì‘ê³  ê°€ë²¼ìš´ ëª¨ë¸ì´ ë” ì •í™•í•œ ì¶”ë¡ ì„ í•˜ë„ë¡ í•™ìŠµì‹œí‚´(Knowledge Distillation, KD)
    - Overfittingë“±ì— ê°•ì¸í•œ ì•™ìƒë¸”(ensemble) ëª¨ë¸ ë° ResNetë“± ë¬´ê±°ìš´ ëª¨ë¸(teacher network)ì—ì„œ distilled knowledgeë¥¼ ì‘ê³  ê°€ë²¼ìš´ ëª¨ë¸(student network)ë¡œ ì „ì´(transfer)
    - ì´ ê³¼ì •ì—ì„œ ì‘ê³  ê°€ë²¼ìš´(shallow/small) student modelì€ ë¬´ê±°ìš´ teacher modelì˜ ì •ë³´ë¥¼ ì „ë‹¬ë°›ì„ ìˆ˜ ìˆìŒ

<center>
<figure>
<img src="/assets/post_img/study/2019-04-16-model_comp_1/fig1.jpg" alt="views">
<figcaption></figcaption>
</figure>
</center>

## Network Distillation methods
### Model compression
- BuciluÇ, C., Caruana, R., & Niculescu-Mizil, A. (2006, August). InÂ Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data miningÂ (pp. 535-541). ACM.
- ê´€ì¸¡ì¹˜(training dataset)ê°€ ë§ë‹¤ë©´ êµ¬ì§€ ì•™ìƒë¸” ëª¨ë¸ì„ ì“°ì§€ ì•Šê³ ë„ ì¼ë°˜í™” ì„±ëŠ¥ì´ ì¢‹ì•„ì§„ë‹¤ëŠ” ì ì—ì„œ ì°©ì•ˆ
  - Heavy model(ensemble)ì€ í•™ìŠµ ë°ì´í„°ì…‹ì´ ë§ì§€ ì•Šì•„ë„ ì¼ë°˜í™” ì„±ëŠ¥ì´ ì¢‹ì•„ ì‹¤ì œ testë‹¨ì—ì„œ ì„±ëŠ¥ì´ ì¢‹ìŒ
    - ì¼ë°˜ì ìœ¼ë¡œ shallow modelì€ training setì—ë§Œ ì í•©í•˜ë„ë¡ overfittingë˜ëŠ” ê²½í–¥ì´ ìˆìŒ
  - Training setì´ ë§ë‹¤ë©´ overfittingì´ ë°œìƒí•˜ë”ë¼ë„ ì „ë°˜ì ìœ¼ë¡œ generalization ì„±ëŠ¥ì´ ì¢‹ì•„ì§
- Training setì„ noiseì„±ë¶„ì„ ì¶”ê°€í•´ oversampling í•˜ì—¬ unlabeled oversampled data ìƒì„±
- Pre-trained heavy modelë¡œ oversampled dataë¥¼ labelingí•˜ì—¬ oversampled dataset ìƒì„±
- Oversampled datasetê³¼ ê¸°ì¡´ì˜ training datasetì„ í•©ì³ large datasetì„ ë§Œë“¤ì–´ light model í•™ìŠµ

<center>
<figure>
<img src="/assets/post_img/study/2019-04-16-model_comp_1/fig2.jpg" alt="views">
<figcaption></figcaption>
</figure>
</center>

- Large datasetì—ëŠ” heavy model(ensemble)ì˜ ì •ë³´ê°€ ë‹´ê²¨ìˆìŒ(distilled knowledge)
- Light modelì˜ large datasetì„ ì´ìš©í•œ í•™ìŠµì„ í†µí•´  robustí•œ ëª¨ë¸ì´ ìƒì„±ë¨
  - More well-generalized / Robust against overfitting

<center>
<figure>
<img src="/assets/post_img/papers/2019-04-02-distilling_knowledge/fig7.jpg" alt="views">
<figcaption>ì‹¤í—˜ ê²°ê³¼</figcaption>
</figure>
</center>

- Result
  - RMSEë¥¼ ì´ìš©í•˜ì—¬ ëª¨ë¸ í‰ê°€
  - Training datasetì˜ í¬ê¸°ê°€ ì»¤ì§ˆìˆ˜ë¡ compressed modelì˜ ì •í™•ë„ê°€ í–¥ìƒë¨
  - Ensemble ëª¨ë¸ê³¼ ì •í™•ë„ê°€ ìœ ì‚¬í•´ì§

### Do deep nets really need to be deep?
- Ba, J., & Caruana, R. (2014). InÂ Advances in neural information processing systemsÂ (pp. 2654-2662).
- Distillation ê³¼ì •ì—ì„œ ë‹¨ìˆœíˆ ë„¤íŠ¸ì›Œí¬ ì¶œë ¥ìœ¼ë¡œ heavy model(ensemble)ì´ ë§Œë“  class ì •ë³´ë¥¼ ì£¼ì—ˆë˜ ì•ì˜ ë°©ì‹ì— logit ê°’ì„ ì ìš©í•˜ì—¬ class information ë¿ë§Œ ì•„ë‹ˆë¼ heavy modelì´ ë§Œë“¤ì–´ë‚´ëŠ” data ë¶„í¬ì •ë³´ê¹Œì§€ ì „ì´
  - Logitì€ í´ë˜ìŠ¤ì˜ ì ìˆ˜ë¡œ, ì´ ì ìˆ˜ë¥¼ í•™ìŠµì‹œì— ê°™ì´ ê³ ë ¤í•˜ì—¬ classì˜ í™•ë¥ ë¶„í¬ë¥¼ ì•Œê²Œ ë¨

<center>
<figure>
<img src="/assets/post_img/study/2019-04-16-model_comp_1/fig3.jpg" alt="views">
<figcaption>Logit</figcaption>
</figure>
</center>

- Logitì´ë€?
  - ìµœì¢… ì¶”ë¡  í›„ activation í†µê³¼ ì „ì˜ ê°’
- í•™ìŠµê³¼ì •
  - Pre-trained heavy model(ensemble)ì˜ logitê°’ì„ light modelë¡œ ì „ì´í•´ì„œ ê·¸ ê°’ì„ í† ëŒ€ë¡œ í•™ìŠµ
- ì´ ê³¼ì •ì„ í†µí•´ ì¶”ë¡ classì˜ ì ìˆ˜(heavy modelì˜ ì¶œë ¥ë¶„í¬)ë¥¼ light modelì´ í•™ìŠµí•˜ê²Œ ë˜ë©° logitì´ ê³§ distilled knowledgeë¡œ ë” ë§ì€ ì •ë³´ë¥¼ ì´ìš©í•˜ì—¬ ë„¤íŠ¸ì›Œí¬ê°€ í•™ìŠµí•˜ê²Œ ë¨

### Deep Model Compression: Distilling Knowledge from Noisy Teachers
- Sau, B. B., & Balasubramanian, V. N. (2016). arXiv preprint arXiv:1610.09650.
- Distillation method 2ì™€ í° í‹€ì—ì„œ ë™ì¼
  - Heavy modelì˜ logitì„ light modelì´ ë‹®ë„ë¡ í•™ìŠµë˜ì–´ì§
- í•˜ì§€ë§Œ heavy model(ensemble)ì˜ logitê°’ì— ì•½ê°„ì˜ noise ì„±ë¶„ì„ ì¶”ê°€
  - Logitì— noiseì„±ë¶„ ğœ–ë¥¼ ì¶”ê°€
- ì¶”ê°€ëœ noise ğœ–ì´ regularizer ì—­í• ì„ í•˜ì—¬ light modelì˜ ì •í™•ë„ê°€ í–¥ìƒë¨

### Distilling the knowledge in a neural network
- Hinton, G., Vinyals, O., & Dean, J. (2015).Â arXiv preprint arXiv:1503.02531.
- ì•ì˜ ë°©ë²•ë“¤(distillation method 2, 3)ê³¼ ë‹¤ë¥´ê²Œ softmaxë¥¼ ì‚¬ìš©
- í•µì‹¬ ì•„ì´ë””ì–´
  - ì¼ë°˜ training datasetìœ¼ë¡œ heavy model(ensemble)ì„ í•™ìŠµì‹œí‚¨ í›„, í•´ë‹¹ datasetì— ëŒ€í•œ predictionì˜ ë¶„í¬ê°’ì„ ì–»ìŒ(probability density function, pdf)
  - ì´ ê³¼ì •ì—ì„œ ì¼ë°˜ì ì¸ neural networkê°€ ì‚¬ìš©í•˜ëŠ” softmaxê°€ ì•„ë‹Œ temperature parameter ğ‘‡ë¥¼ ì ìš©ì‹œí‚¨ customized softmaxë¥¼ ì‚¬ìš©
    - ì¼ë°˜ softmaxëŠ” ê°€ì¥ í° ê°’ì„ 1ì— ë§¤ìš° ê°€ê¹ê²Œ, ë‚˜ë¨¸ì§€ë¥¼ 0ì— ë§¤ìš° ê°€ê¹ë„ë¡ mapping(one-hot hard target)
    - Customized softmaxëŠ” temperature parameter ğ‘‡ì— ë”°ë¼ 1ê³¼ 0 ì‚¬ì´ì˜ soft targetsë¥¼ ì¶œë ¥
      - ğ‘‡ê°€ 0ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ì¼ë°˜ hard target ìƒì„±(ğ‘‡=1 ì¼ ë•Œ ì¼ë°˜ softmax)
      - ğ‘‡ê°€ 10ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ëª¨ë“  í™•ë¥ ì€ 1/# of class ë¡œ ìˆ˜ë ´í•´ ì œëŒ€ë¡œëœ ì¶”ë¡  ë¶ˆê°€
      - ğ‘‡ê°€ ì¤‘ê°„ì— ì ì ˆí•˜ë„ë¡ ì„¤ì •í•˜ì—¬ soft targetsë¥¼ ìƒì„±í•˜ë„ë¡ í•˜ì—¬ì•¼ í•¨(ì‹¤í—˜ì ìœ¼ë¡œ ì°¾ì•„ì•¼í•¨)
    - Pre-trained heavy networkê°€ customized softmaxë¥¼ ì´ìš©í•˜ì—¬ ìƒì„±í•œ pdfë¥¼ ìƒì„±í•˜ë„ë¡ light modelì´ í•™ìŠµ
      - ë…¼ë¬¸ì—ì„  ğ‘‡ë¡œ 2~5 ì‚¬ì´ì˜ ê°’ ì¤‘ ì ì ˆí•œ ê°’ì„ ì°¾ì•„ ì‚¬ìš©(datasetë³„ë¡œ)
      - ì‹¤ì œ loss functionì€ ì¼ë°˜ì ì¸ softmaxë¡œ ì¶”ë¡ ëœ label ì •ë³´ì™€ soft targets ì •ë³´ ëª¨ë‘ í•™ìŠµì„ ìœ„í•œ ë³€í˜•ëœ cross entropy lossê°€ ì‚¬ìš©ë¨
        - Cross entropy lossê°€ label ì •ë³´ì™€ soft target ì •ë³´ í™œìš©ì„ ìœ„í•œ 2ê°œ termìœ¼ë¡œ ë‚˜ë‰˜ì–´ ìˆìœ¼ë©°, ë¹„ìœ¨ì´ ğ›¼ë¡œ ì¡°ì ˆë¨(ğ›¼=0.5)
- ì‹¤í—˜ ê²°ê³¼
  - ì‹¤í—˜ ê²°ê³¼, ì¼ë°˜ì ì¸ ê²½ìš°ì— ë¹„í•´ soft targetì´ ì ìš©ëœ ëª¨ë¸ì´ ë” ì •í™•í•¨
  
<center>
<figure>
<img src="/assets/post_img/study/2019-04-16-model_comp_1/fig4.jpg" alt="views">
<figcaption>ì‹¤í—˜ ê²°ê³¼</figcaption>
</figure>
</center>

## Network Distillation on SSD
### Object detection at 200 Frames Per Second
- Mehta, Rakesh, and Cemalettin Ozturk.Â Proceedings of the European Conference on Computer Vision (ECCV). 2018
- ì •í™•ë„ê°€ ë†’ì€ teacher networkì˜ ì •ë³´ë¥¼ distillation loss, unlabeled dataë¥¼ ì´ìš©í•˜ì—¬ light-weight student networkì— ì „ë‹¬
- Keypoint
  - Tiny-Yolo(Yolo-v2)ë¥¼ ë³€í˜•í•˜ì—¬ íš¨ìœ¨ì ì¸ êµ¬ì¡° ì œì•ˆ
  - Deep/narrow êµ¬ì¡°ë¥¼ ì´ìš©í•˜ì—¬ ì •í™•ë„ëŠ” ë³´ì¡´í•˜ë©´ì„œ ì—°ì‚°ëŸ‰ ì¤„ì„
  - Distillation loss
  - Training data
- Main contributions
  - R-CNN ê³„ì—´ì— ëŒ€í•œ distillation ì ìš©ì´ ì•„ë‹Œ sing shot ë°©ì‹ì˜ detectorì— ëŒ€í•œ ì ìš©
  - Single shot detectorê°€ ë„¤íŠ¸ì›Œí¬ì˜ ì¶”ë¡  í›„ ì™¸ë¶€ì˜ non-maximal suppression(NMS)ë¥¼ ì‚¬ìš©í•˜ì—¬ ìµœì¢… ì¶œë ¥ì„ ë‚¸ë‹¤ëŠ” ê²ƒì„ ì°¸ê³ í•˜ì—¬ FM-NMSë¥¼ ì œì•ˆí•˜ê³ , ì´ë¡œë¶€í„° teacherì˜ distilled knowledgeë¥¼ studentë¡œ ì „ì´
  - Objectness scaled distillation lossë¥¼ ì œì•ˆí•˜ê³ , ì´ë¥¼ ì´ìš©í•˜ì—¬ distillationì„ ìˆ˜í–‰
- Dense feature map stacking
  - Feature mapì„ poolingì„ ì‚¬ìš©í•˜ì—¬ concatí•˜ëŠ” ë°©ì‹ì´ ì•„ë‹Œ yolo-v2ì˜ feature map stacking ì‚¬ìš©
    - ì¼ë°˜ì ì¸ max poolingì€ ì •ë³´ì˜ ì†ì‹¤ì´ ë°œìƒí•˜ë¯€ë¡œ í° feature mapì„ resizeí•˜ì—¬ feature mapì˜ activationì´ ë‹¤ë¥¸ feature mapìœ¼ë¡œ ë¶„ì‚°ë˜ë„ë¡ í•¨.
    - Bottleneckì„ ì ìš©í•´ depthë¥¼ ì¶”ê°€í•˜ì—¬ ë„¤íŠ¸ì›Œí¬ ìš©ëŸ‰(capacity)ì€ ëŠ˜ë¦¬ë©´ì„œ ì—°ì‚°ëŸ‰ê³¼ íŒŒë¼ë¯¸í„° ìˆ˜ë¥¼ ë³´ì¡´
- Deep but narrow
  - Feature extractorì˜ ì¶œë ¥ ê°œìˆ˜ë¥¼ ì¤„ì´ê³  feature stackingì„ ì´ìš©
  - Narrowí•˜ê³  deepí•˜ë„ë¡ 1x1 conv layerë¥¼ ì´ìš©í•˜ì—¬ ì—°ì‚°ëŸ‰ì„ ë³´ì¡´
- Distillation loss for Training
  - ë„¤íŠ¸ì›Œí¬ì˜ ìµœì¢… ì¶œë ¥ì€ ë„ˆë¬´ ë§ì€ ì •ë³´ë¥¼ ë‹´ê³ ìˆìŒ
    - NMSë¥¼ ê±°ì¹˜ê¸° ì „ì˜ feature mapì—ì„œ ê°ì²´ í´ë˜ìŠ¤ ì •ë³´ì™€ ìœ„ì¹˜ ì •ë³´ê°€ í˜¼ì¡í•˜ê²Œ ì¡´ì¬
    - ì´ë¥¼ ê·¸ëŒ€ë¡œ distillationí•  ê²½ìš° studentëŠ” overfittingìœ¼ë¡œ ì¸í•´ ì •í™•ë„ê°€ ì˜¤íˆë ¤ í•˜ë½í•¨
  - ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ studentì˜ bounding box coordinateì™€ class probabilityê°€ teacher predictionì˜ objetness valueê°€ ë†’ì„ë•Œë§Œ distilled ì •ë³´ë¥¼ í•™ìŠµí•˜ë„ë¡ í•¨
    - í•™ìŠµ ê³¼ì •ì—ì„œ loss functionì— ì˜í•´ teacherê°€ backgroundë¡œ ë¶„ë¥˜ë˜ëŠ” ê°ì²´ì—ê²Œ ë§¤ìš° ì‘ì€  objectness valueë¥¼ í• ë‹¹í•˜ë„ë¡ í•™ìŠµë¨

<center>
<figure>
<img src="/assets/post_img/papers/2019-04-11-object_detection_200fps/fig3.jpg" alt="views">
<figcaption></figcaption>
</figure>
</center>

- Feature Map-NMS(FM-NMS)
  - ë™ì¼ ê°ì²´ê°€ ì¡´ì¬í• ê²ƒì´ë¼ ë„¤íŠ¸ì›Œí¬ê°€ ì¶”ë¡ í•˜ëŠ” KxK cell ì§€ì—­ì— multiple candidateê°€ ì¡´ì¬í•œë‹¤ë©´ í•˜ë‚˜ì˜ ê°ì²´ì¼ í™•ë¥ ì´ í¼
  - ì´ ì¤‘ ê°€ì¥ ë†’ì€ objectness valueë¥¼ ê°–ëŠ” ê²°ê³¼ë¥¼ ì„ íƒí•˜ì—¬ ê°ì²´ë¡œ í•¨.
- Effectiveness of data
  - Labeled dataê°€ ë§ì•„ì§ˆ ë•Œ ë„¤íŠ¸ì›Œí¬ì˜ ì •í™•ë„ê°€ ì–¼ë§ˆë‚˜ ê°œì„ ë˜ë‚˜ í™•ì¸
    - PASCAL VOCì™€ í•´ë‹¹ í´ë˜ìŠ¤ì˜ MS COCO ì´ìš©
  - Unlabeled dataì— ëŒ€í•´ì„  teacherì˜ output(soft label)ë§Œì„ ì´ìš©í•˜ì—¬ studentë¥¼ í•™ìŠµ

<center>
<figure>
<img src="/assets/post_img/papers/2019-04-11-object_detection_200fps/table1.jpg" alt="views">
<img src="/assets/post_img/papers/2019-04-11-object_detection_200fps/table3.jpg" alt="views">
<figcaption></figcaption>
</figure>
</center>

- ì‹¤í—˜ ê²°ê³¼(table 1, 3)
  - Merging different layers
    - ë‹¤ì–‘í•œ layerì—ì„œ feature mapì´ stacking ë  ë•Œ ì •í™•ë„ê°€ ë” í–¥ìƒë¨
    - ì´ˆê¸° ë ˆì´ì–´ë³´ë‹¨ ë’¤ìª½ ë ˆì´ì–´ë¥¼ stackingí•˜ëŠ”ê²Œ ì •í™•ë„ í–¥ìƒì— ë„ì›€ë¨
    - Max poolingë³´ë‹¤ stackingì´ ë” ì •í™•ë„ê°€ í–¥ìƒë¨
  - Distillation with labeled data
    - PASCAL VOC 2007, 2012ì™€ COCOì—ì„œì˜ 65K ì¶”ê°€ ì´ë¯¸ì§€
    - ì‹¤í—˜ ê²°ê³¼, distillationì„ ì ìš©í•˜ê¸° ìœ„í•´ì„  distillation loss functionì˜ objetness value scalingì´ í•„ìˆ˜ë¡œ í•„ìš”í•¨(Obj-scaling)
    - í•™ìŠµ ë°ì´í„°ì…‹ì´ ë§ì„ìˆ˜ë¡ ë” ë§ì€ soft labelì´ ì´ìš© ê°€ëŠ¥í•´ì§€ë¯€ë¡œ studentì˜ ì •í™•ë„ í–¥ìƒì´ ì»¸ìŒ
    - í•˜ì§€ë§Œ teacherì˜ ì„±ëŠ¥ì€ studentì˜ ì„±ëŠ¥í–¥ìƒì— í¬ê²Œ ì˜í–¥ì„ ë¼ì¹˜ì§€ ëª»í•¨(COCO teacherì˜ ì •í™•ë„ì— ìƒê´€ ì—†ì´ studentëŠ” ëª¨ë‘ ì •í™•ë„ê°€ ë¹„ìŠ·í•˜ê²Œ í–¥ìƒ)
  - Unlabeled data
    - COCO unlabeled dataë¥¼ ëŠ˜ë ¤ê°€ë©° ì‹¤í—˜ì„ ìˆ˜í–‰
    - ì¶”ê°€ dataê°€ ìˆì„ ê²½ìš° ì„±ëŠ¥ì´ í–¥ìƒë˜ëŠ”ê²ƒì„ í™•ì¸ ê°€ëŠ¥

<center>
<figure>
<img src="/assets/post_img/papers/2019-04-11-object_detection_200fps/table5.jpg" alt="views">
<img src="/assets/post_img/papers/2019-04-11-object_detection_200fps/table6.jpg" alt="views">
<figcaption></figcaption>
</figure>
</center>

## Knowledge distillationì´ ì™œ ë™ì‘í• ê¹Œ?
- https://blog.lunit.io/2018/03/22/distilling-the-knowledge-in-a-neural-network-nips-2014-workshop/
- ì¼ë°˜ì ìœ¼ë¡œ ë„¤íŠ¸ì›Œí¬ë¥¼ í•™ìŠµì‹œí‚¤ëŠ”ë°ëŠ” softmaxë¥¼ ê±°ì¹œ hard labelì„ ì‚¬ìš©í•¨
- í•˜ì§€ë§Œ softmaxë¥¼ ê±°ì¹˜ê¸° ì „ì—ëŠ” ê°ê° ë ˆì´ë¸”ë³„ë¡œ í™•ë¥ ë¶„í¬ë¥¼ ê°–ë„ë¡ ë˜ëŠ” ë” ë§ì€ ì •ë³´ë¥¼ ë‹´ê³  ìˆìŒ

<center>
<figure>
<img src="/assets/post_img/study/2019-04-16-model_comp_1/fig5.jpg" alt="views">
<figcaption></figcaption>
</figure>
</center>

- ì¼ë°˜í™” ì„±ëŠ¥ì´ ì¢‹ì€, ì˜ í•™ìŠµëœ ë„¤íŠ¸ì›Œí¬ê°€ ìœ„ì˜ ì˜ìƒì„ ê°ê° dogì¼ í™•ë¥ ì„ 90%, catì¼ í™•ë¥ ì„ 10%ë¡œ ì¶”ë¡ í•  ê²½ìš°, softmaxë¥¼ ê±°ì¹˜ë©´ dogì¼ í™•ë¥  99%ë¡œ ë‹¨ìˆœíšŒ ë¨
- ì¦‰, ì¼ë°˜ì ì¸ softmaxë¥¼ ê±°ì¹œ hard labelë³´ë‹¤ softmaxë¥¼ ê±°ì¹˜ê¸° ì „ ê° í´ë˜ìŠ¤ë³„ í™•ë¥  ë¶„í¬ì— ëŒ€í•œ ì •ë³´ë¥¼ ë‹´ê³  ìˆëŠ” ê²½ìš°ê°€ í›¨ì”¬ ë” ë§ì€ ì •ë³´ë¥¼ ë‹®ê³  ìˆìŒ
  - ì˜ˆë¥¼ ë“¤ì–´ ê°•ì•„ì§€ë¥¼ 90%ë‹®ê³ , ê³ ì–‘ì´ë¥¼ 10%ë‹®ê³ , ì•„ì£¼ ì‘ì€ í™•ë¥ ê³  ì†Œì˜ í„¸ ëì„ ë‹®ê³ , ìë™ì°¨ì˜ ì–´ë–¤ ë¶€ë¶„ì„ ë‹®ê³ ..
- ìœ„ì™€ ê°™ì€ ì¶”ê°€ì ì¸ ì •ë³´(knowledge)ë¥¼ soft targetì´ ë‹®ê³  ìˆê³ , ì´ëŸ¬í•œ í™•ë¥ ë¶„í¬ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ student networkë¡œ ì „ë‹¬(transfer)ì‹œì¼œì£¼ê¸° ë•Œë¬¸ì— KDê°€ student networkì˜ ì„±ëŠ¥ì„ í–¥ìƒì‹œì¼œ ì£¼ëŠ” ê²ƒì„
