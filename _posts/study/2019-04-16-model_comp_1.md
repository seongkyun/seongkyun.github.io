---
layout: post
title: Model compression(Pruning/Distillation) ì •ë¦¬
category: study
tags: [Model compression, Pruning, Distillation]
comments: true
---

# Model compression(Pruning/Distillation) ì •ë¦¬

## Model Compression
- __Model compression?__ í° ëª¨ë¸ì„ ì‘ê²Œ ë§Œë“œëŠ” ë°©ë²•
  - ëª¨ë¸ì˜ íš¨ìœ¨ì„ ì¢‹ê²Œ í•˜ë„ë¡ redundancyë¥¼ ì¤„ì´ê±°ë‚˜ efficiencyë¥¼ ë†’ì´ëŠ” ë°©ì‹
- Pruning / Distillation
  - Pruning: ëª¨ë¸ì—ì„œ ì¤‘ìš”ë„ê°€ ë‚®ì€ ë‰´ëŸ°ì„ ì œê±°í•˜ì—¬ ì§ì ‘ì ì¸ ëª¨ë¸ í¬ê¸°ì˜ ê°ì†Œ
    - ëª¨ë¸ì˜ redundancyë¥¼ ì¤„ì´ëŠ” ë°©ë²•
    - ì¥ì : ëª¨ë¸ ìµœì í™”ì— ë”°ë¥¸ ì—°ì‚°ëŸ‰ ê°ì†Œ ë° ëª¨ë¸ì‚¬ì´ì¦ˆ ì¶•ì†Œìœ¨ì´ ìƒë‹¹í•¨
    - ë‹¨ì : ëª¨ë¸ í•™ìŠµ ë° pruning ê³¼ì •ì´ ë³µì¡í•˜ê³  ë§¤ìš° ì˜¤ë˜ ê±¸ë¦¬ë©°, taskë³„ë¡œ ë‹¤ë¥¸ ì ìš©ë°©ë²•ì„ í•„ìš”ë¡œí•¨

  - Distillation: ì‘ì€ ëª¨ë¸ì— í° ëª¨ë¸ì˜ ì •ë³´(knowledge)ë¥¼ ì „ë‹¬í•˜ì—¬ ì‘ì€ ëª¨ë¸ì˜ ì •í™•ë„ë¥¼ í–¥ìƒì‹œí‚´
    - ëª¨ë¸ì˜ efficiencyë¥¼ ë†’ì´ëŠ” ë°©ì‹
    - ì¥ì : Pre-trained modelì„ ì´ìš©í•´ ë¹ ë¥´ê²Œ í•™ìŠµì‹œí‚¬ ìˆ˜ ìˆìœ¼ë©° ë²”ìš©ì ìœ¼ë¡œ ì ìš© ê°€ëŠ¥
    - ë‹¨ì : ëª¨ë¸ ìµœì í™” ê°œë…ë³´ë‹¤ëŠ” ì¡´ì¬í•˜ëŠ” ëª¨ë¸ì˜ í‘œí˜„ë ¥(capacity)ì„ ìµœëŒ€í•œ ì‚¬ìš©í•˜ê²Œ ë˜ì–´ ì •í™•ë„ê°€ í–¥ìƒë˜ì–´ í° ëª¨ë¸ì„ ëŒ€ì²´í•  ìˆ˜ ìˆê²Œ ë˜ëŠ” ê°„ì ‘ì ì¸ ëª¨ë¸ ì‚¬ì´ì¦ˆ ì¶•ì†Œë°©ë²•

## Network Pruning
- ë„¤íŠ¸ì›Œí¬ì˜ ìˆ˜ë§ì€ íŒŒë¼ë¯¸í„°ì¤‘ ì¤‘ìš”í•˜ì§€ ì•Šì€ íŒŒë¼ë¯¸í„°ë“¤ì„ ì œê±°í•˜ëŠ” ë°©ë²•
  - ì–´ë– í•œ ê¸°ì¤€ì„ ì •í•˜ì—¬ rankë¥¼ ë§Œë“¤ë„ë¡ ë‰´ëŸ°ì„ ì •ë ¬(ranking)
  - Rankingì— ë”°ë¼ ì¼ì • thresholdë¥¼ ë„˜ì§€ ëª»í•˜ëŠ” ë‰´ëŸ°ì„ ì œê±°
    - ì´ ê³¼ì •ì—ì„œ ë’¤ì— ì—°ê²°ëœ ë‰´ëŸ°ì—ê¹Œì§€ ëª¨ë‘ ì˜í–¥ì„ ë¯¸ì¹˜ë¯€ë¡œ íŒŒë¼ë¯¸í„°ê°€ ë§ì´ ì œê±°ë¨(pruning)
    - Pruned ë„¤íŠ¸ì›Œí¬ì˜ ì •í™•ë„ëŠ” ì „ì—ë¹„í•´ ì¡°ê¸ˆ ê°ì†Œë¨
    - ë„ˆë¬´ ë§ì´ pruningì´ ë˜ë©´ ë„¤íŠ¸ì›Œí¬ê°€ ì†ìƒë˜ì–´ ì •í™•ë„ íšŒë³µì´ ë¶ˆê°€ëŠ¥
  - Pruning í›„ ë„¤íŠ¸ì›Œí¬ë¥¼ ì¬í•™ìŠµì‹œì¼œ ì •í™•ë„ë¥¼ íšŒë³µ
    - Interactive Pruning

<center>
<figure>
<img src="/assets/post_img/study/2019-04-13-pruning/fig1.png" alt="views">
<figcaption>Interactive pruning</figcaption>
</figure>
</center>

- Pruning Convolutional Neural Networks for Resource Efficient Inference
  - ICLR 2017 ë…¼ë¬¸(Nvidia)
  - Pre-trained VGG16ì˜ filterë¥¼ randomí•˜ê²Œ ê³¨ë¼ pruningí•˜ê³  validation setì—ì„œ cost functionì˜ ë³€í™”ë¥¼ ì œì¼ ì ê²Œ í•˜ëŠ” ë‰´ëŸ°ì˜ rankê°€ ë†’ë„ë¡ í•˜ì—¬ ë‚˜ì—´
  - ì‹¤í—˜ê²°ê³¼ VGG16 ë„¤íŠ¸ì›Œí¬ ëª¨ë¸ì‚¬ì´ì¦ˆ 10ë°°ì •ë„ ì¤„ì¼ ìˆ˜ ìˆì—ˆìŒ
    - Caltech Birds-200 dataset ì´ìš©

## Network Pruning on SSD
- __Multi-layer Pruning Framework for Compressing Single Shot MultiBox Detector__
  - Singh, Pravendra, et al. 2019 IEEE Winter Conference on Applications of Computer Vision (WACV). IEEE, 2019.
  - Single Shot Multi-Box Detector(SSD)ë¥¼ pruning
    - Object detection: PASCAL VOC datasetì— ëŒ€í•´ ì •í™•ë„ëŠ” ìœ ì§€í•˜ë©° 6.7~4.9ë°°ì˜ ëª¨ë¸ ì••ì¶•(VGG16 based)
    - Classification: CIFAR datasetì— ëŒ€í•´ ì •í™•ë„ëŠ” ìœ ì§€í•˜ë©° 125ë°°ì˜ ëª¨ë¸ ì••ì¶•(VGG16)
  1. Sparsity induction: Pre-trained SSD ë¥¼ L1-normì´ ì ìš©ëœ loss functionìœ¼ë¡œ threshold ì´í•˜ì˜ weightë¥¼ 0ìœ¼ë¡œ ë§Œë“¤ì–´ pruning ë  layer ì§‘í•© ğ¿ì„ ë§Œë“¦
    - L1-normì—ëŠ” test setê³¼ val setì„ ì´ìš©í•¨
    - ê¸°ì¤€ì´ ë  thresholdëŠ” validation setì„ ì´ìš©í•˜ì—¬ í•´ë‹¹ ë ˆì´ì–´ì˜ ê°’ í‰ê· ì„ ê¸°ì¤€ìœ¼ë¡œ ì •í•¨
  2. Filter selection: ì´ë ‡ê²Œ ì„ íƒëœ layer ì§‘í•© ğ¿ì˜ ë ˆì´ì–´ ğ‘™ì˜ ì¤‘ìš”ë„ë¥¼ í‰ê°€
    - ë ˆì´ì–´ ğ‘™ê³¼ ë‹¤ìŒ ë ˆì´ì–´ ğ‘™+1ê°„ì˜ filter sparsity staticsë¥¼ ì´ìš©í•˜ì—¬ ì¤‘ìš”ë„ í‰ê°€
    - ì´ ê³¼ì •ì—ì„œ ë ˆì´ì–´ lì—ì„œ ì¤‘ìš”í•˜ì§€ ì•Šë‹¤ê³  íŒë‹¨ë˜ëŠ” filterì˜ listë¥¼ ì–»ìŒ
    - Layer ì§‘í•© ğ¿ì— ëŒ€í•´ ëª¨ë‘ ë°˜ë³µìˆ˜í–‰
  3. Pruning: ì•ì—ì„œ ì¤‘ìš”í•˜ì§€ ì•Šë‹¤ íŒë‹¨ëœ ë ˆì´ì–´ ğ‘™ê³¼ í•´ë‹¹ outputê³¼ ì—°ê²°ëœ layer ğ‘™+1ì„ ì œê±°
    - Layer ì§‘í•© ğ¿ì— ëŒ€í•´ ëª¨ë‘ ë°˜ë³µìˆ˜í–‰
  4. Retraining: ì•ì—ì„œ pruningëœ ë„¤íŠ¸ì›Œí¬ë¥¼ original loss(SSD)ë¥¼ ì´ìš©í•´ ë–¨ì–´ì§„ ì •í™•ë„ë¥¼ ë³µì›(ì¬í•™ìŠµ)

### ì‹¤í—˜ ê²°ê³¼

<center>
<figure>
<img src="/assets/post_img/papers/2019-04-15-pruning_ssd/table1.jpg" alt="views">
</figure>
</center>

- ì‹¤í—˜ ê²°ê³¼ ì •í™•ë„ëŠ” ë¹„êµì  ë³´ì¡´í•˜ë©´ì„œ ë„¤íŠ¸ì›Œí¬ì˜ í¬ê¸°ëŠ” ë§¤ìš° í¬ê²Œ ì¤„ì–´ë“¤ê²Œ ë¨

<center>
<figure>
<img src="/assets/post_img/papers/2019-04-15-pruning_ssd/table3.jpg" alt="views">
<img src="/assets/post_img/papers/2019-04-15-pruning_ssd/table5.jpg" alt="views">
<figcaption></figcaption>
</figure>
</center>

- ë˜í•œ ì—¬íƒ€ ëª¨ë¸ì— ë¹„í•´ í›¨ì”¬ ë†’ì€ mAPë¡œ ì¶”ë¡ í•˜ë©´ì„œë„ í›¨ì”¬ ì‘ì€ íŒŒë¼ë¯¸í„°ìˆ˜ë¥¼ ê°€ì§
- SSD300ì˜ ê²½ìš° detection layerë¥¼ ë’·ë‹¨ì—ì„œ pruning í•˜ëŠ”ê²ƒì´ ì•ë‹¨ì—ì„œ í•˜ëŠ”ê²ƒë³´ë‹¤ ë” ì •í™•í•œ ì‹¤í—˜ ê²°ê³¼ë¥¼ ë³´ì´ëŠ” ê²ƒì„ í™•ì¸ í•  ìˆ˜ ìˆì—ˆëŠ”ë°, ì´ëŠ” êµ¬ì¡°ìƒ 4, 5, 6ë²ˆì§¸ detection layerì˜ parameter ìˆ˜ê°€ ë§¤ìš° ì ì§€ë§Œ í° ê°ì²´ë¥¼ ì°¾ëŠ”ë° ì£¼ìš”í•˜ê²Œ ì‘ìš©í•˜ê¸° ë•Œë¬¸ì„

## Network Distillation
- ê°œë°œì— ì‚¬ìš©ë˜ëŠ” ë”¥ëŸ¬ë‹ ë„¤íŠ¸ì›Œí¬ì™€ ì‹¤ì œ ì ìš©ë˜ëŠ” ëª¨ë¸ì€ ë‹¤ë¦„
  - ê°œë°œì—ëŠ” ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ì´ ììœ ë¡­ì§€ë§Œ, applicationë‹¨ì—ì„œëŠ” ê·¸ë ‡ì§€ ì•ŠìŒ
  - ì´ë¥¼ ìœ„í•´ í¬ê³  ë¬´ê±°ìš´ ëª¨ë¸ì˜ ì •ë³´(knowledge)ë¥¼ ì‘ê³  ê°€ë²¼ìš´ ëª¨ë¸ë¡œ ì „ë‹¬í•˜ì—¬ ì‘ê³  ê°€ë²¼ìš´ ëª¨ë¸ì´ ë” ì •í™•í•œ ì¶”ë¡ ì„ í•˜ë„ë¡ í•™ìŠµì‹œí‚´(Knowledge Distillation, KD)
    - Overfittingë“±ì— ê°•ì¸í•œ ì•™ìƒë¸”(ensemble) ëª¨ë¸ ë° ResNetë“± ë¬´ê±°ìš´ ëª¨ë¸(teacher network)ì—ì„œ distilled knowledgeë¥¼ ì‘ê³  ê°€ë²¼ìš´ ëª¨ë¸(student network)ë¡œ ì „ì´(transfer)
    - ì´ ê³¼ì •ì—ì„œ ì‘ê³  ê°€ë²¼ìš´(shallow/small) student modelì€ ë¬´ê±°ìš´ teacher modelì˜ ì •ë³´ë¥¼ ì „ë‹¬ë°›ì„ ìˆ˜ ìˆìŒ

<center>
<figure>
<img src="/assets/post_img/study/2019-04-16-model_comp_1/fig1.jpg" alt="views">
<figcaption></figcaption>
</figure>
</center>

## Network Distillation methods
### Model compression
- BuciluÇ, C., Caruana, R., & Niculescu-Mizil, A. (2006, August). InÂ Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data miningÂ (pp. 535-541). ACM.
- ê´€ì¸¡ì¹˜(training dataset)ê°€ ë§ë‹¤ë©´ êµ¬ì§€ ì•™ìƒë¸” ëª¨ë¸ì„ ì“°ì§€ ì•Šê³ ë„ ì¼ë°˜í™” ì„±ëŠ¥ì´ ì¢‹ì•„ì§„ë‹¤ëŠ” ì ì—ì„œ ì°©ì•ˆ
  - Heavy model(ensemble)ì€ í•™ìŠµ ë°ì´í„°ì…‹ì´ ë§ì§€ ì•Šì•„ë„ ì¼ë°˜í™” ì„±ëŠ¥ì´ ì¢‹ì•„ ì‹¤ì œ testë‹¨ì—ì„œ ì„±ëŠ¥ì´ ì¢‹ìŒ
    - ì¼ë°˜ì ìœ¼ë¡œ shallow modelì€ training setì—ë§Œ ì í•©í•˜ë„ë¡ overfittingë˜ëŠ” ê²½í–¥ì´ ìˆìŒ
  - Training setì´ ë§ë‹¤ë©´ overfittingì´ ë°œìƒí•˜ë”ë¼ë„ ì „ë°˜ì ìœ¼ë¡œ generalization ì„±ëŠ¥ì´ ì¢‹ì•„ì§
- Training setì„ noiseì„±ë¶„ì„ ì¶”ê°€í•´ oversampling í•˜ì—¬ unlabeled oversampled data ìƒì„±
- Pre-trained heavy modelë¡œ oversampled dataë¥¼ labelingí•˜ì—¬ oversampled dataset ìƒì„±
- Oversampled datasetê³¼ ê¸°ì¡´ì˜ training datasetì„ í•©ì³ large datasetì„ ë§Œë“¤ì–´ light model í•™ìŠµ

<center>
<figure>
<img src="/assets/post_img/study/2019-04-16-model_comp_1/fig2.jpg" alt="views">
<figcaption></figcaption>
</figure>
</center>

- Large datasetì—ëŠ” heavy model(ensemble)ì˜ ì •ë³´ê°€ ë‹´ê²¨ìˆìŒ(distilled knowledge)
- Light modelì˜ large datasetì„ ì´ìš©í•œ í•™ìŠµì„ í†µí•´  robustí•œ ëª¨ë¸ì´ ìƒì„±ë¨
  - More well-generalized / Robust against overfitting

<center>
<figure>
<img src="/assets/post_img/papers/2019-04-02-distilling_knowledge/fig7.jpg" alt="views">
<figcaption>ì‹¤í—˜ ê²°ê³¼</figcaption>
</figure>
</center>

- Result
  - RMSEë¥¼ ì´ìš©í•˜ì—¬ ëª¨ë¸ í‰ê°€
  - Training datasetì˜ í¬ê¸°ê°€ ì»¤ì§ˆìˆ˜ë¡ compressed modelì˜ ì •í™•ë„ê°€ í–¥ìƒë¨
  - Ensemble ëª¨ë¸ê³¼ ì •í™•ë„ê°€ ìœ ì‚¬í•´ì§

### Do deep nets really need to be deep?
- Ba, J., & Caruana, R. (2014). InÂ Advances in neural information processing systemsÂ (pp. 2654-2662).
- Distillation ê³¼ì •ì—ì„œ ë‹¨ìˆœíˆ ë„¤íŠ¸ì›Œí¬ ì¶œë ¥ìœ¼ë¡œ heavy model(ensemble)ì´ ë§Œë“  class ì •ë³´ë¥¼ ì£¼ì—ˆë˜ ì•ì˜ ë°©ì‹ì— logit ê°’ì„ ì ìš©í•˜ì—¬ class information ë¿ë§Œ ì•„ë‹ˆë¼ heavy modelì´ ë§Œë“¤ì–´ë‚´ëŠ” data ë¶„í¬ì •ë³´ê¹Œì§€ ì „ì´
  - Logitì€ í´ë˜ìŠ¤ì˜ ì ìˆ˜ë¡œ, ì´ ì ìˆ˜ë¥¼ í•™ìŠµì‹œì— ê°™ì´ ê³ ë ¤í•˜ì—¬ classì˜ í™•ë¥ ë¶„í¬ë¥¼ ì•Œê²Œ ë¨

<center>
<figure>
<img src="/assets/post_img/study/2019-04-16-model_comp_1/fig3.jpg" alt="views">
<figcaption>Logit</figcaption>
</figure>
</center>

- Logitì´ë€?
  - ìµœì¢… ì¶”ë¡  í›„ activation í†µê³¼ ì „ì˜ ê°’
- í•™ìŠµê³¼ì •
  - Pre-trained heavy model(ensemble)ì˜ logitê°’ì„ light modelë¡œ ì „ì´í•´ì„œ ê·¸ ê°’ì„ í† ëŒ€ë¡œ í•™ìŠµ
- ì´ ê³¼ì •ì„ í†µí•´ ì¶”ë¡ classì˜ ì ìˆ˜(heavy modelì˜ ì¶œë ¥ë¶„í¬)ë¥¼ light modelì´ í•™ìŠµí•˜ê²Œ ë˜ë©° logitì´ ê³§ distilled knowledgeë¡œ ë” ë§ì€ ì •ë³´ë¥¼ ì´ìš©í•˜ì—¬ ë„¤íŠ¸ì›Œí¬ê°€ í•™ìŠµí•˜ê²Œ ë¨

### Deep Model Compression: Distilling Knowledge from Noisy Teachers
- Sau, B. B., & Balasubramanian, V. N. (2016). arXiv preprint arXiv:1610.09650.
- Distillation method 2ì™€ í° í‹€ì—ì„œ ë™ì¼
  - Heavy modelì˜ logitì„ light modelì´ ë‹®ë„ë¡ í•™ìŠµë˜ì–´ì§
- í•˜ì§€ë§Œ heavy model(ensemble)ì˜ logitê°’ì— ì•½ê°„ì˜ noise ì„±ë¶„ì„ ì¶”ê°€
  - Logitì— noiseì„±ë¶„ ğœ–ë¥¼ ì¶”ê°€
- ì¶”ê°€ëœ noise ğœ–ì´ regularizer ì—­í• ì„ í•˜ì—¬ light modelì˜ ì •í™•ë„ê°€ í–¥ìƒë¨



