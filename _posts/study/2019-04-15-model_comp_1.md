---
layout: post
title: Model compression(Pruning/Distillation) 정리
category: study
tags: [Model compression, Pruning, Distillation]
comments: true
---

# Model compression(Pruning/Distillation) 정리

## Model Compression
- __Model compression?__ 큰 모델을 작게 만드는 방법
  - 모델의 효율을 좋게 하도록 redundancy를 줄이거나 efficiency를 높이는 방식
- Pruning / Distillation
  - Pruning: 모델에서 중요도가 낮은 뉴런을 제거하여 직접적인 모델 크기의 감소
    - 모델의 redundancy를 줄이는 방법
    - 장점: 모델 최적화에 따른 연산량 감소 및 모델사이즈 축소율이 상당함
    - 단점: 모델 학습 및 pruning 과정이 복잡하고 매우 오래 걸리며, task별로 다른 적용방법을 필요로함

  - Distillation: 작은 모델에 큰 모델의 정보(knowledge)를 전달하여 작은 모델의 정확도를 향상시킴
    - 모델의 efficiency를 높이는 방식
    - 장점: Pre-trained model을 이용해 빠르게 학습시킬 수 있으며 범용적으로 적용 가능
    - 단점: 모델 최적화 개념보다는 존재하는 모델의 표현력(capacity)을 최대한 사용하게 되어 정확도가 향상되어 큰 모델을 대체할 수 있게 되는 간접적인 모델 사이즈 축소방법

## Network Pruning
- 네트워크의 수많은 파라미터중 중요하지 않은 파라미터들을 제거하는 방법
  - 어떠한 기준을 정하여 rank를 만들도록 뉴런을 정렬(ranking)
  - Ranking에 따라 일정 threshold를 넘지 못하는 뉴런을 제거
    - 이 과정에서 뒤에 연결된 뉴런에까지 모두 영향을 미치므로 파라미터가 많이 제거됨(pruning)
    - Pruned 네트워크의 정확도는 전에비해 조금 감소됨
    - 너무 많이 pruning이 되면 네트워크가 손상되어 정확도 회복이 불가능
  - Pruning 후 네트워크를 재학습시켜 정확도를 회복
    - Interactive Pruning

<center>
<figure>
<img src="/assets/post_img/study/2019-04-13-pruning/fig1.png" alt="views">
<figcaption>Interactive pruning</figcaption>
</figure>
</center>

- Pruning Convolutional Neural Networks for Resource Efficient Inference
  - ICLR 2017 논문(Nvidia)
  - Pre-trained VGG16의 filter를 random하게 골라 pruning하고 validation set에서 cost function의 변화를 제일 적게 하는 뉴런의 rank가 높도록 하여 나열
  - 실험결과 VGG16 네트워크 모델사이즈 10배정도 줄일 수 있었음
    - Caltech Birds-200 dataset 이용

## Network Pruning on SSD
- __Multi-layer Pruning Framework for Compressing Single Shot MultiBox Detector__
  - Singh, Pravendra, et al. 2019 IEEE Winter Conference on Applications of Computer Vision (WACV). IEEE, 2019.
  - Single Shot Multi-Box Detector(SSD)를 pruning
    - Object detection: PASCAL VOC dataset에 대해 정확도는 유지하며 6.7~4.9배의 모델 압축(VGG16 based)
    - Classification: CIFAR dataset에 대해 정확도는 유지하며 125배의 모델 압축(VGG16)
  1. Sparsity induction: Pre-trained SSD 를 L1-norm이 적용된 loss function으로 threshold 이하의 weight를 0으로 만들어 pruning 될 layer 집합 𝐿을 만듦
    - L1-norm에는 test set과 val set을 이용함
    - 기준이 될 threshold는 validation set을 이용하여 해당 레이어의 값 평균을 기준으로 정함
  2. Filter selection: 이렇게 선택된 layer 집합 𝐿의 레이어 𝑙의 중요도를 평가
    - 레이어 𝑙과 다음 레이어 𝑙+1간의 filter sparsity statics를 이용하여 중요도 평가
    - 이 과정에서 레이어 l에서 중요하지 않다고 판단되는 filter의 list를 얻음
    - Layer 집합 𝐿에 대해 모두 반복수행
  3. Pruning: 앞에서 중요하지 않다 판단된 레이어 𝑙과 해당 output과 연결된 layer 𝑙+1을 제거
    - Layer 집합 𝐿에 대해 모두 반복수행
  4. Retraining: 앞에서 pruning된 네트워크를 original loss(SSD)를 이용해 떨어진 정확도를 복원(재학습)

### 실험 결과

<center>
<figure>
<img src="/assets/post_img/study/2019-04-13-pruning/table1.jpg" alt="views">
</figure>
</center>

- 실험 결과 정확도는 비교적 보존하면서 네트워크의 크기는 매우 크게 줄어들게 됨

<center>
<figure>
<img src="/assets/post_img/study/2019-04-13-pruning/table3.jpg" alt="views">
<img src="/assets/post_img/study/2019-04-13-pruning/table5.jpg" alt="views">
<figcaption></figcaption>
</figure>
</center>

- 또한 여타 모델에 비해 훨씬 높은 mAP로 추론하면서도 훨씬 작은 파라미터수를 가짐
- SSD300의 경우 detection layer를 뒷단에서 pruning 하는것이 앞단에서 하는것보다 더 정확한 실험 결과를 보이는 것을 확인 할 수 있었는데, 이는 구조상 4, 5, 6번째 detection layer의 parameter 수가 매우 적지만 큰 객체를 찾는데 주요하게 작용하기 때문임

## Network Distillation
- 개발에 사용되는 딥러닝 네트워크와 실제 적용되는 모델은 다름
  - 개발에는 리소스 사용이 자유롭지만, application단에서는 그렇지 않음
  - 이를 위해 크고 무거운 모델의 정보(knowledge)를 작고 가벼운 모델로 전달하여 작고 가벼운 모델이 더 정확한 추론을 하도록 학습시킴(Knowledge Distillation, KD)
    - Overfitting등에 강인한 앙상블(ensemble) 모델 및 ResNet등 무거운 모델(teacher network)에서 distilled knowledge를 작고 가벼운 모델(student network)로 전이(transfer)
    - 이 과정에서 작고 가벼운(shallow/small) student model은 무거운 teacher model의 정보를 전달받을 수 있음

<center>
<figure>
<img src="/assets/post_img/study/2019-04-15-model_comp_1/fig1.jpg" alt="views">
<figcaption></figcaption>
</figure>
</center>

## Network Distillation method 1
- __Model compression__
  - Buciluǎ, C., Caruana, R., & Niculescu-Mizil, A. (2006, August). In Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining (pp. 535-541). ACM.
  - 관측치(training dataset)가 많다면 구지 앙상블 모델을 쓰지 않고도 일반화 성능이 좋아진다는 점에서 착안
    - Heavy model(ensemble)은 학습 데이터셋이 많지 않아도 일반화 성능이 좋아 실제 test단에서 성능이 좋음
      - 일반적으로 shallow model은 training set에만 적합하도록 overfitting되는 경향이 있음
    - Training set이 많다면 overfitting이 발생하더라도 전반적으로 generalization 성능이 좋아짐
  - Training set을 noise성분을 추가해 oversampling 하여 unlabeled oversampled data 생성
  - Pre-trained heavy model로 oversampled data를 labeling하여 oversampled dataset 생성
  - Oversampled dataset과 기존의 training dataset을 합쳐 large dataset을 만들어 light model 학습
  
<center>
<figure>
<img src="/assets/post_img/study/2019-04-15-model_comp_1/fig2.jpg" alt="views">
<figcaption></figcaption>
</figure>
</center>
